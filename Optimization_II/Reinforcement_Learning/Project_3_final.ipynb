{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNVBEEI-zis9"
      },
      "source": [
        "# Optimization 2 - Project 3: Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "camw_py8JDgq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Input, Conv3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9t-EyACMRvg",
        "outputId": "2cc72c4b-dce4-4cc6-af60-cb45473dd842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "108U9ZOFMcDh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# modify the location where the ROMs are saved on your GoogleDrive\n",
        "!python -m atari_py.import_roms /content/drive/MyDrive/Roms/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "psSsaPDUJDg1"
      },
      "outputs": [],
      "source": [
        "def prepro(I):\n",
        "    # preprocess each frame for learning\n",
        "    # save some memory and computation\n",
        "    # pre-process the image from a 210x160x3 uint8 frame into an (80x80) float array \n",
        "    I = I[35:195,:,:].copy() # crop the top of the image...score image doesn't matter for how to play\n",
        "    I = I[::2,::2,0].copy()\n",
        "    I[I == 144] = 0 # erase background (background type 1)\n",
        "    I[I == 109] = 0 # erase background (background type 2)\n",
        "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    return np.array(I.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nn6hDx4YJDg6"
      },
      "outputs": [],
      "source": [
        "# Only needed for policy gradient, approach. \n",
        "def discount_rewards(r,delt):\n",
        "    \"\"\"Send in an array 'r' of scores and a discount factor 'delt'. This function will return and array of discounted rewards.\"\"\"\n",
        "    # take 1D float array of rewards and compute discounted reward\n",
        "    # gym returns a reward with every single frame.  most of those rewards are 0\n",
        "    # sometimes they're 1 or -1 if we win or lose a point in that specific frame\n",
        "    # we want non-0 rewards for every frame. \n",
        "    # so take each frame, figure out if we eventually won the corresponding point or not\n",
        "    # if so make the reward positive, if not negative\n",
        "    # but more recent actions (relative to the frame where the point is awarded) are more \n",
        "    # impactful to the score that frames a long time ago, so discount rewards...\n",
        "    \n",
        "    nr = len(r)\n",
        "    # we want to change all those zeros into discounted values of the next reward (this is the value function!)\n",
        "    discounted_r = [0.0]*nr\n",
        "    \n",
        "    for t in range(nr):\n",
        "        # start at the end\n",
        "        if r[nr-t-1] > 0: # if you won a point in this frame we want a good reward\n",
        "            discounted_r[nr-t-1] = 1\n",
        "        elif r[nr-t-1] < 0: # if we lost the point we want a bad reward\n",
        "            discounted_r[nr-t-1] = -1\n",
        "        elif t==0: # this is just for error catching...at t==0 r[nr-t-1] should have already been + or -...\n",
        "            discounted_r[nr-t-1] = 0\n",
        "        elif discounted_r[nr-t-1] == 0: # otherwise you want to look at the next reward value and discount it\n",
        "            discounted_r[nr-t-1] = delt*discounted_r[nr-t]\n",
        "    return discounted_r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N1onESTCyoiM"
      },
      "outputs": [],
      "source": [
        "def move_toward_ball(pixels:np.array) -> int:\n",
        "    \"\"\"This function returns the action necessary to move towards the ball.\"\"\"\n",
        "\n",
        "    r_pad_y = np.where(pixels[:,71])[0].mean() # Where is our paddle\n",
        "    ball_p_range = pixels[:,10:70] # What is the range of pixels the ball could be in\n",
        "\n",
        "    if ball_p_range.sum() < 1:\n",
        "        action = 0 # If the ball isn't in our range, we don't do anything\n",
        "    else:\n",
        "        ball_col = np.where(ball_p_range.sum(axis=0))[0][0]\n",
        "        ball_y = np.where(ball_p_range[:,ball_col])[0].mean()\n",
        "\n",
        "        if ball_y > r_pad_y:\n",
        "            action = 3\n",
        "        elif ball_y < r_pad_y:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 0\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY82-vDx0RCn"
      },
      "source": [
        "Below cells depict a few of the modeling arhitectures we chosen, because it was the one that was the least error prone, but we thought it helpful to keep these here so that we have a record of some of the other options chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_mXligTNJDg9"
      },
      "outputs": [],
      "source": [
        "def create_model(height,width,channels):\n",
        "    # we cannot simply have 3 output nodes because we want to put a weight on each node's impact to the objective\n",
        "    # that is different for each data point.  the only way to achieve this is to have 3 output layers, each having 1 node\n",
        "    # the effect is the same, just the way TF/keras handles weights is different\n",
        "    imp = Input(shape=(height,width,channels))\n",
        "    mid = Flatten()(imp)\n",
        "    mid = Dense(16,activation='relu',kernel_regularizer = tf.keras.regularizers.l1(0.001))(mid)\n",
        "    mid = Dense(16,activation='relu',kernel_regularizer = tf.keras.regularizers.l1(0.001))(mid)\n",
        "    mid = Dense(16,activation='relu',kernel_regularizer = tf.keras.regularizers.l1(0.001))(mid)\n",
        "    mid = Dense(16,activation='relu',kernel_regularizer = tf.keras.regularizers.l1(0.001))(mid)\n",
        "    mid = Dense(8,activation='relu',kernel_regularizer = tf.keras.regularizers.l1(0.001))(mid)\n",
        "    out0 = Dense(3,activation='softmax')(mid)\n",
        "    model = Model(imp,out0) \n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.00003),loss='sparse_categorical_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_model(height,width,channels):\n",
        "    # we cannot simply have 3 output nodes because we want to put a weight on each node's impact to the objective\n",
        "    # that is different for each data point.  the only way to achieve this is to have 3 output layers, each having 1 node\n",
        "    # the effect is the same, just the way TF/keras handles weights is different\n",
        "    imp = Input(shape=(height,width,channels))\n",
        "    mid = Flatten()(imp)\n",
        "    mid = Dense(16,activation='elu',kernel_regularizer = tf.keras.regularizers.l1(0.0001),kernel_initializer='he_uniform')(mid)\n",
        "    mid = Dense(4,activation='elu',kernel_regularizer = tf.keras.regularizers.l1(0.0001),kernel_initializer='he_uniform')(mid)\n",
        "    out0 = Dense(3,activation='softmax',kernel_initializer='he_uniform')(mid)\n",
        "    model = Model(imp,out0) \n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=.00003),loss='sparse_categorical_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_model(height,width,channels):\n",
        "    # we cannot simply have 3 output nodes because we want to put a weight on each node's impact to the objective\n",
        "    # that is different for each data point.  the only way to achieve this is to have 3 output layers, each having 1 node\n",
        "    # the effect is the same, just the way TF/keras handles weights is different\n",
        "    imp = Input(shape=(height,width,channels))\n",
        "    mid = Conv2D(16,(8,8),strides=4,activation='relu')(imp)\n",
        "    mid = Conv2D(32,(4,4),strides=2,activation='relu')(mid)\n",
        "    mid = Flatten()(mid)\n",
        "    mid = Dense(256,activation='relu')(mid)\n",
        "    mid = Dense(128,activation='relu')(mid)\n",
        "    out0 = Dense(3,activation='softmax')(mid)\n",
        "    model = Model(imp,out0) \n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),loss='sparse_categorical_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# frames_to_net = 4   # how many previous frames will we feed the NN\n",
        "# possible_actions = [0,2,3] # Do Nothing, Down, Up\n",
        "\n",
        "# mod = create_model(80,80,frames_to_net)\n",
        "# # mod = tf.keras.models.load_model('drive/MyDrive/Opti/mod.tf')\n",
        "# mod.call = tf.function(mod.call,experimental_relax_shapes=True)\n",
        "# mod.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames_to_net = 4   # how many previous frames will we feed the NN\n",
        "possible_actions = [0,2,3] # Do Nothing, Down, Up"
      ],
      "metadata": {
        "id": "cAJSYOiT62SE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh8DL3GmCkdN"
      },
      "source": [
        "### Gameplaying Mechanics\n",
        "The below function is used to simulate playing one game. It take three arguments: a model to make predictions with, an epsisol_rand, which indicates the likelihood of choosing a random action, and an epsilon_track, which indicates the likelihood of choosing the action that will move the paddle closest to the ball.\n",
        "\n",
        "We have set up this function so that our model will only make a decision every four frames and that decision is persisted through for the following four frames. This provides a more human like, flow to the gameplay, rather than having a jerky paddle. Additionally, we adjusted how frames, rewards, adn actions are recorded. Instead of saving the feed and reward for every action, we store these values in chunks. The \"frame_chunk\" that we save is the feed that was used to make the decision for the following four frames. We then save the actions that were chosen over the next four frames and the total rewards that were accumulated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GRLU-FM98oRZ"
      },
      "outputs": [],
      "source": [
        "def play_game(model, epsilon_rand,epsilon_track):\n",
        "    \"\"\"Send in a model to make decisions with\"\"\"\n",
        "\n",
        "    env0 = gym.make(\"Pong-v0\")\n",
        "    pix = env0.reset()\n",
        "    pix = prepro(pix)\n",
        "    frames_this_game = 0\n",
        "    feed = np.zeros((1,80,80,frames_to_net)) # store past four frames. Intially they are blank. 0 index is most recent frame. 3 index is oldest.\n",
        "    feed[0,:,:,0] = pix.copy() \n",
        "    \n",
        "    frame_chunks = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    \n",
        "    score = 0\n",
        "    done = False\n",
        "    fcount = 0\n",
        "    chunk_reward = 0\n",
        "\n",
        "    decision_feed = feed.copy()\n",
        "    action_input = 0\n",
        "\n",
        "    # Play the game until it is over.\n",
        "    while not done:\n",
        "        \n",
        "        if fcount == 0: # If we are back to fcount of 0 it means we need to make a new decision for next four frames\n",
        "            if np.random.random() < epsilon_track: # We might track the ball \n",
        "                action0 = move_toward_ball(pix)\n",
        "            elif np.random.random() < epsilon_rand: # Or we may just be quirky and random\n",
        "                action0 = np.random.choice(possible_actions)\n",
        "            else:\n",
        "                vf = model(feed,training=False).numpy()[0] # Get predicted probabilities for best action?\n",
        "                # action = np.random.choice(3,p=vf) # Randomly choose an action based on those probs\n",
        "                action0 = np.random.choice(possible_actions,p=vf)\n",
        "                fcount += 1\n",
        "\n",
        "            if frames_this_game > 0:\n",
        "                \n",
        "                # Store The most recent rewards with their old decisions\n",
        "                rewards.append(chunk_reward)\n",
        "                frame_chunks.append(decision_feed)\n",
        "                actions.append(action_input)\n",
        "\n",
        "                # Reset the variables for next time\n",
        "                decision_feed = feed.copy()\n",
        "                action_input = possible_actions.index(action0)\n",
        "                chunk_reward = 0 # Reset the rewards for this feed chunk\n",
        "\n",
        "\n",
        "        elif fcount == 3:\n",
        "            fcount = 0\n",
        "        else:\n",
        "            fcount += 1\n",
        "\n",
        "        frames_this_game += 1\n",
        "\n",
        "        # Take our action and get the results!\n",
        "        pix_new, reward, done, info = env0.step(action0)\n",
        "        \n",
        "        score += reward # Total Score\n",
        "        chunk_reward += reward # Cumulative Score for these four chunks\n",
        "\n",
        "        pix = prepro(pix_new) # Process the new pixels for the next step\n",
        "    \n",
        "        # Shift the feed backward and add in new frame\n",
        "        feed[0,:,:,1:] = feed[0,:,:,:frames_to_net-1].copy()\n",
        "        feed[0,:,:,0] = pix.copy()\n",
        "\n",
        "        if done: # Don't forget to store the last point\n",
        "            rewards.append(chunk_reward)\n",
        "            frame_chunks.append(decision_feed)\n",
        "            actions.append(action_input)\n",
        "\n",
        "        \n",
        "    return frame_chunks, actions, rewards, score\n",
        "\n",
        "# frames, actions, rewards, score =  play_game(mod,0,0)\n",
        "# score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5gtU-Nr1Jvw"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbZN_XEp68oz",
        "outputId": "9ef3cfe0-05f3-4cdc-a04e-4918d9a86b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80, 80, 4)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 19, 19, 16)        4112      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 32)          8224      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 570,163\n",
            "Trainable params: 570,163\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Uncomment out the below line if you want to load in an already saved model\n",
        "mod = tf.keras.models.load_model('drive/MyDrive/Opti/mod.tf')\n",
        "mod.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOIMYRTv1NYD"
      },
      "source": [
        "#### Setting Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TXFCL6EwJDhI"
      },
      "outputs": [],
      "source": [
        "ngames = 15_000 # How many games to train for\n",
        "nbatch = 10 # How many batches per epoch\n",
        "buffn = 30_000 # How many frame chunks do we want to keep in memory? # Collab can only handle so much.\n",
        "warmupgames = 15 \n",
        "delt= .99 # \n",
        "epochs = 1\n",
        "epochs = 2\n",
        "\n",
        "# What should our probabilities for ball tracking be. We start with 100% chance of ball tracking for the first 5% of games. Then decrease it down to 0 over the next 5% of games.\n",
        "# Our hope is that this will allow our model to jump start the learning process by examining many successful points.\n",
        "# eps_tracking = [1]*int(ngames*.05) + list(np.linspace(1,0,int(ngames*.05))) # Possibility for ball tracking for first 30% of ga,es\n",
        "# How often should we choose a random action:\n",
        "# eps_rand = list(np.linspace(.05,0,int(ngames*.90)))\n",
        "eps_rand = []\n",
        "eps_tracking = []\n",
        "\n",
        "len_buff = 0\n",
        "buffer = {'frame_chunks':[],'actions':[],'rewards':[]}\n",
        "# print(len(eps_tracking))\n",
        "# print(len(eps_rand))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvP9fpTYKiUG"
      },
      "outputs": [],
      "source": [
        "rolling_score_total = 0\n",
        "for game in range(ngames):\n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    # Select the appropriate epsilon values for this game.\n",
        "    if game < len(eps_tracking):\n",
        "        epsilon_track = eps_tracking[game]\n",
        "    else:\n",
        "        epsilon_track = 0\n",
        "    if game < len(eps_rand):\n",
        "        epsilon_rand = eps_rand[game]\n",
        "    else:\n",
        "        epsilon_rand = 0\n",
        "\n",
        "    frame_chunks, actions, rewards, score = play_game(mod,epsilon_rand,epsilon_track) # Get results of that one game.\n",
        "    rewards = discount_rewards(rewards.copy(),delt) # Every single one should have a number. But numbers that are far away from the point will be small.\n",
        "\n",
        "    # print(f'More Buffing after game = {game}')\n",
        "    # Add in the new results from the game\n",
        "    buffer['frame_chunks'] += frame_chunks.copy()\n",
        "    buffer['actions'] += actions.copy()\n",
        "    buffer['rewards'] += rewards.copy()\n",
        "    len_buff += len(actions)\n",
        "\n",
        "    if len_buff > buffn: \n",
        "        # Clean out the older frames if necessary\n",
        "        excess = len_buff - buffn\n",
        "        buffer['frame_chunks'] = buffer['frame_chunks'][excess:].copy()\n",
        "        buffer['actions'] = buffer['actions'][excess:].copy()\n",
        "        buffer['rewards'] = buffer['rewards'][excess:].copy()\n",
        "        len_buff = len(buffer['actions'])\n",
        "\n",
        "\n",
        "    if game >= warmupgames: # After the game is played, we want to update the neural neural network with a random sample from our buffer\n",
        "        # Set up the inputs and outputs that we will need to feed into the net. \n",
        "        frames_to_sample = len(frame_chunks)*2 # We would like a random same of frame chunks, rewards and actions from our buffer.\n",
        "\n",
        "        rewards = np.zeros(frames_to_sample)\n",
        "        actions = np.zeros(frames_to_sample)\n",
        "        current_frames = np.zeros((frames_to_sample,80,80,frames_to_net))\n",
        "\n",
        "        prob = np.ones(len_buff)\n",
        "        prob[np.array(buffer['rewards']) > 0] = 10.0 # Weight positive rewards higher than negative rewards.\n",
        "        prob /= np.sum(prob)\n",
        "        which_choose = np.random.choice(len_buff,size=frames_to_sample,replace=False,p=prob) # Pick a random sample of frames in the buffer to train on.\n",
        "    \n",
        "        for grab in range(frames_to_sample):\n",
        "            rewards[grab] = buffer['rewards'][which_choose[grab]]\n",
        "            actions[grab] = buffer['actions'][which_choose[grab]]\n",
        "            current_frames[grab] = buffer['frame_chunks'][which_choose[grab]]\n",
        "       \n",
        "        # Update neural network\n",
        "        mod.fit(current_frames,actions,epochs=epochs,steps_per_epoch=nbatch,verbose=0,sample_weight=rewards,use_multiprocessing=True)\n",
        "\n",
        "    stop = time.time()\n",
        "    rolling_score_total += score\n",
        "    print(game, score, stop-start,len_buff)\n",
        "    \n",
        "    if game % 50 == 0: # Save model every 50 games in awhile\n",
        "        print(f'Average Score for past 50 games: {rolling_score_total/50}') \n",
        "        rolling_score_total = 0\n",
        "        mod.save('drive/MyDrive/Opti/mod.tf')\n",
        "\n",
        "# mod.save('drive/MyDrive/Opti/mod.tf')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkS_qGq1AK2G"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw90FwFh8X5H"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWvpFdQw8Xxw"
      },
      "source": [
        "## Testing the Trained Model\n",
        "\n",
        "Now we test the model trained above to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "43KL780k8XUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2f49ce-eca4-448f-b4cc-41134dc5af61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-17.77"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "mod_trained = tf.keras.models.load_model('drive/MyDrive/Opti/mod.tf')\n",
        "scores = []\n",
        "for game in range(200):\n",
        "    # Play the game with this model. Both epsilons should be 0 now.\n",
        "    frame_chunks, actions, rewards, score = play_game(mod_trained,0,0) \n",
        "    scores.append(score)\n",
        "\n",
        "scores = np.array(scores)\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nSNZ_ntJacHM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "99d0bd44-5666-454b-a78d-1d24b60c6706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns; sns.set_theme()\n",
        "\n",
        "\n",
        "sns.histplot(scores)\n",
        "plt.title('Scores of final model in 200 games.')\n",
        ";"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5gdtjpu2o0a",
        "outputId": "7cba57f5-a904-4b12-e11f-4a48676147b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3aaM03ppacDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fc9630-c6a9-41fd-e032-775100c9964d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-17.77"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71K9t9te4ZsU"
      },
      "source": [
        "### Model Training Part 2:\n",
        "Same as above, but now we only train on 5,000 games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rj3i4mo4vu5"
      },
      "outputs": [],
      "source": [
        "mod_2 = create_model(80,80,frames_to_net)\n",
        "mod_2.call = tf.function(mod_2.call,experimental_relax_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_q6cPRS4Zsq"
      },
      "outputs": [],
      "source": [
        "ngames = 5000\n",
        "nbatch = 10 # How many batches per epoch\n",
        "buffn = 30_000 # How many frame chunks do we want to keep in memory? # Collab can only handle so much.\n",
        "warmupgames = 15 \n",
        "delt= .99\n",
        "epochs = 1\n",
        "\n",
        "eps_tracking = [1]*int(ngames*.05) + list(np.linspace(1,0,int(ngames*.05))) # Possibility for ball tracking for first 30% of ga,es\n",
        "eps_rand = list(np.linspace(.05,0,int(ngames*.90)))\n",
        "eps_rand = []\n",
        "\n",
        "len_buff = 0\n",
        "buffer = {'frame_chunks':[],'actions':[],'rewards':[]}\n",
        "print(len(eps_tracking))\n",
        "print(len(eps_rand))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYe6enPJ4Zsr"
      },
      "outputs": [],
      "source": [
        "rolling_score_total = 0\n",
        "for game in range(ngames):\n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    # Select the appropriate epsilon values for this game.\n",
        "    if game < len(eps_tracking):\n",
        "        epsilon_track = eps_tracking[game]\n",
        "    else:\n",
        "        epsilon_track = 0\n",
        "    if game < len(eps_rand):\n",
        "        epsilon_rand = eps_rand[game]\n",
        "    else:\n",
        "        epsilon_rand = 0\n",
        "\n",
        "    frame_chunks, actions, rewards, score = play_game(mod_2,epsilon_rand,epsilon_track) # Get results of that one game.\n",
        "    rewards = discount_rewards(rewards.copy(),delt) # Every single one should have a number. But numbers that are far away from the point will be small.\n",
        "\n",
        "    # print(f'More Buffing after game = {game}')\n",
        "    # Add in the new results from the game\n",
        "    buffer['frame_chunks'] += frame_chunks.copy()\n",
        "    buffer['actions'] += actions.copy()\n",
        "    buffer['rewards'] += rewards.copy()\n",
        "    len_buff += len(actions)\n",
        "\n",
        "    if len_buff > buffn: \n",
        "        # Clean out the older frames if necessary\n",
        "        excess = len_buff - buffn\n",
        "        buffer['frame_chunks'] = buffer['frame_chunks'][excess:].copy()\n",
        "        buffer['actions'] = buffer['actions'][excess:].copy()\n",
        "        buffer['rewards'] = buffer['rewards'][excess:].copy()\n",
        "        len_buff = len(buffer['actions'])\n",
        "\n",
        "\n",
        "    if game >= warmupgames: # After the game is played, we want to update the neural neural network with a random sample from our buffer\n",
        "        # Set up the inputs and outputs that we will need to feed into the net. \n",
        "        frames_to_sample = len(frame_chunks) # We would like a random same of frame chunks, rewards and actions from our buffer.\n",
        "\n",
        "        rewards = np.zeros(frames_to_sample)\n",
        "        actions = np.zeros(frames_to_sample)\n",
        "        current_frames = np.zeros((frames_to_sample,80,80,frames_to_net))\n",
        "\n",
        "        prob = np.ones(len_buff)\n",
        "        prob[np.array(buffer['rewards']) > 0] = 10.0 # Weight positive rewards higher than negative rewards.\n",
        "        prob /= np.sum(prob)\n",
        "        which_choose = np.random.choice(len_buff,size=frames_to_sample,replace=False,p=prob) # Pick a random sample of frames in the buffer to train on.\n",
        "    \n",
        "        for grab in range(frames_to_sample):\n",
        "            rewards[grab] = buffer['rewards'][which_choose[grab]]\n",
        "            actions[grab] = buffer['actions'][which_choose[grab]]\n",
        "            current_frames[grab] = buffer['frame_chunks'][which_choose[grab]]\n",
        "       \n",
        "        # Update neural network\n",
        "        mod_2.fit(current_frames,actions,epochs=epochs,steps_per_epoch=nbatch,verbose=0,sample_weight=rewards,use_multiprocessing=True)\n",
        "\n",
        "    stop = time.time()\n",
        "    rolling_score_total += score\n",
        "    print(game, score, stop-start,len_buff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ih-faiu4Zss"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSXvpGj74Zss"
      },
      "source": [
        "## Testing the Trained Model 2\n",
        "\n",
        "Now we test the model trained above to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO533Fx94Zss"
      },
      "outputs": [],
      "source": [
        "scores2 = []\n",
        "for game in range(200):\n",
        "    # Play the game with this model. Both epsilons should be 0 now.\n",
        "    frame_chunks, actions, rewards, scores2 = play_game(mod_2,0,0) \n",
        "    scores2.append(scores2)\n",
        "\n",
        "scores2 = np.array(scores2)\n",
        "scores2.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcEJCjbF4Zss"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHOBtu-14Zss"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1c-iRPDab9Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V74hFb18cebI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QRQkhtCOS7v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1U_WMIJhgi8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYLe4F2FxTn-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Project_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}